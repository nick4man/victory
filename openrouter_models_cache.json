{
  "generated_at": "2026-02-03T00:00:00Z",
  "source": "https://openrouter.ai/api/v1/models",
  "models": [
    {
      "id": "openrouter/free",
      "canonical_slug": "openrouter/free",
      "hugging_face_id": "",
      "name": "Free Models Router",
      "created": 1769917427,
      "description": "The simplest way to get free inference. openrouter/free is a router that selects free models at random from the models available on OpenRouter. The router smartly filters for models that support features needed for your request such as image understanding, tool calling, structured outputs and more.\n",
      "context_length": 200000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Router",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "stepfun/step-3.5-flash:free",
      "canonical_slug": "stepfun/step-3.5-flash",
      "hugging_face_id": "stepfun-ai/Step-3.5-Flash",
      "name": "StepFun: Step 3.5 Flash (free)",
      "created": 1769728337,
      "description": "Step 3.5 Flash is StepFun's most capable open-source foundation model.\nBuilt on a sparse Mixture of Experts (MoE) architecture, it selectively activates only 11B of its 196B parameters per token.\nIt is a reasoning model that is incredibly speed efficient even at long contexts.",
      "context_length": 256000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 256000,
        "max_completion_tokens": 256000,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "stop",
        "temperature",
        "tools",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "arcee-ai/trinity-large-preview:free",
      "canonical_slug": "arcee-ai/trinity-large-preview",
      "hugging_face_id": "arcee-ai/Trinity-Large-Preview",
      "name": "Arcee AI: Trinity Large Preview (free)",
      "created": 1769552670,
      "description": "Trinity-Large-Preview is a frontier-scale open-weight language model from Arcee, built as a 400B-parameter sparse Mixture-of-Experts with 13B active parameters per token using 4-of-256 expert routing.\n\nIt excels in creative writing, storytelling, role-play, chat scenarios, and real-time voice assistance, better than your average reasoning model usually can. But we’re also introducing some of our newer agentic performance. It was trained to navigate well in agent harnesses like OpenCode, Cline, and Kilo Code, and to handle complex toolchains and long, constraint-filled prompts.\n\nThe architecture natively supports very long context windows up to 512k tokens, with the Preview API currently served at 128k context using 8-bit quantization for practical deployment.\nTrinity-Large-Preview reflects Arcee’s efficiency-first design philosophy, offering a production-oriented frontier model with open weights and permissive licensing suitable for real-world applications and experimentation.",
      "context_length": 131000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 131000,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "structured_outputs",
        "temperature",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 0.8,
        "top_p": 0.8,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "moonshotai/kimi-k2.5",
      "canonical_slug": "moonshotai/kimi-k2.5-0127",
      "hugging_face_id": "moonshotai/Kimi-K2.5",
      "name": "MoonshotAI: Kimi K2.5",
      "created": 1769487076,
      "description": "Kimi K2.5 is Moonshot AI's native multimodal model, delivering state-of-the-art visual coding capability and a self-directed agent swarm paradigm.\nBuilt on Kimi K2 with continued pretraining over approximately 15T mixed visual and text tokens, it delivers strong performance in general reasoning, visual coding, and agentic tool-calling.",
      "context_length": 262144,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000005",
        "completion": "0.0000028"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "upstage/solar-pro-3:free",
      "canonical_slug": "upstage/solar-pro-3",
      "hugging_face_id": "",
      "name": "Upstage: Solar Pro 3 (free)",
      "created": 1769481200,
      "description": "Solar Pro 3 is Upstage's powerful Mixture-of-Experts (MoE) language model.\nWith 102B total parameters and 12B active parameters per forward pass, it delivers exceptional performance while maintaining computational efficiency.\nOptimized for Korean with English and Japanese support.",
      "context_length": 128000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "response_format",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": "2026-03-02"
    },
    {
      "id": "minimax/minimax-m2-her",
      "canonical_slug": "minimax/minimax-m2-her-20260123",
      "hugging_face_id": "",
      "name": "MiniMax: MiniMax M2-her",
      "created": 1769177239,
      "description": "MiniMax M2-her is a dialogue-first large language model built for immersive roleplay, character-driven chat, and expressive multi-turn conversations.\nDesigned to stay consistent in tone and personality, it supports rich message roles (user_system, group, sample_message_user, sample_message_ai) and can learn from example dialogue to better match the style and pacing of your scenario, making it a strong choice for storytelling, companions, and conversational experiences where natural flow and vivid interaction matter most.",
      "context_length": 65536,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000003",
        "completion": "0.0000012",
        "input_cache_read": "0.00000003"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": 2048,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "temperature",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "writer/palmyra-x5",
      "canonical_slug": "writer/palmyra-x5-20250428",
      "hugging_face_id": "",
      "name": "Writer: Palmyra X5",
      "created": 1769003823,
      "description": "Palmyra X5 is Writer's most advanced model, purpose-built for building and scaling AI agents across the enterprise.\nIt delivers industry-leading speed and efficiency on context windows up to 1 million tokens, powered by a novel transformer architecture and hybrid attention mechanisms.\nThis enables faster inference and expanded memory for processing large volumes of enterprise data, critical for scaling AI agents.",
      "context_length": 1040000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000006",
        "completion": "0.000006"
      },
      "top_provider": {
        "context_length": 1040000,
        "max_completion_tokens": 8192,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "liquid/lfm-2.5-1.2b-thinking:free",
      "canonical_slug": "liquid/lfm-2.5-1.2b-thinking-20260120",
      "hugging_face_id": "LiquidAI/LFM2.5-1.2B-Thinking",
      "name": "LiquidAI: LFM2.5-1.2B-Thinking (free)",
      "created": 1768927527,
      "description": "LFM2.5-1.2B-Thinking is a lightweight reasoning-focused model optimized for agentic tasks, data extraction, and RAG—while still running comfortably on edge devices.\nIt supports long context (up to 32K tokens) and is designed to provide higher-quality “thinking” responses in a small 1.2B model.",
      "context_length": 32768,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 32768,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "liquid/lfm-2.5-1.2b-instruct:free",
      "canonical_slug": "liquid/lfm-2.5-1.2b-instruct-20260120",
      "hugging_face_id": "LiquidAI/LFM2.5-1.2B-Instruct",
      "name": "LiquidAI: LFM2.5-1.2B-Instruct (free)",
      "created": 1768927521,
      "description": "LFM2.5-1.2B-Instruct is a compact, high-performance instruction-tuned model built for fast on-device AI.\nIt delivers strong chat quality in a 1.2B parameter footprint, with efficient edge inference and broad runtime support.",
      "context_length": 32768,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 32768,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-audio",
      "canonical_slug": "openai/gpt-audio",
      "hugging_face_id": "",
      "name": "OpenAI: GPT Audio",
      "created": 1768862569,
      "description": "The gpt-audio model is OpenAI's first generally available audio model.\nThe new snapshot features an upgraded decoder for more natural sounding voices and maintains better voice consistency.\nAudio is priced at $32 per million input tokens and $64 per million output tokens.",
      "context_length": 128000,
      "architecture": {
        "modality": "text+audio->text+audio",
        "input_modalities": [
          "text",
          "audio"
        ],
        "output_modalities": [
          "text",
          "audio"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000025",
        "completion": "0.00001",
        "audio": "0.000032"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": 16384,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-audio-mini",
      "canonical_slug": "openai/gpt-audio-mini",
      "hugging_face_id": "",
      "name": "OpenAI: GPT Audio Mini",
      "created": 1768859419,
      "description": "A cost-efficient version of GPT Audio.\nThe new snapshot features an upgraded decoder for more natural sounding voices and maintains better voice consistency.\nInput is priced at $0.60 per million tokens and output is priced at $2.40 per million tokens.",
      "context_length": 128000,
      "architecture": {
        "modality": "text+audio->text+audio",
        "input_modalities": [
          "text",
          "audio"
        ],
        "output_modalities": [
          "text",
          "audio"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000006",
        "completion": "0.0000024",
        "audio": "0.0000006"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": 16384,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "z-ai/glm-4.7-flash",
      "canonical_slug": "z-ai/glm-4.7-flash-20260119",
      "hugging_face_id": "zai-org/GLM-4.7-Flash",
      "name": "Z.AI: GLM 4.7 Flash",
      "created": 1768833913,
      "description": "As a 30B-class SOTA model, GLM-4.7-Flash offers a new option that balances performance and efficiency.\nIt is further optimized for agentic coding use cases, strengthening coding capabilities, long-horizon task planning, and tool collaboration, and has achieved leading performance among open-source models of the same size on several current public benchmark leaderboards.",
      "context_length": 200000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000007",
        "completion": "0.0000004",
        "input_cache_read": "0.00000001"
      },
      "top_provider": {
        "context_length": 200000,
        "max_completion_tokens": 131072,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": 1,
        "top_p": 0.95,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-5.2-codex",
      "canonical_slug": "openai/gpt-5.2-codex-20260114",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-5.2-Codex",
      "created": 1768409315,
      "description": "GPT-5.2-Codex is an upgraded version of GPT-5.1-Codex optimized for software engineering and coding workflows.\nIt is designed for both interactive development sessions and long, independent execution of complex engineering tasks. Compared to GPT-5.1-Codex, 5.2-Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter.\nRead the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup.",
      "context_length": null,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000006",
        "completion": "0.000005"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-5.2",
      "canonical_slug": "openai/gpt-5.2-20260202",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-5.2",
      "created": 1768409315,
      "description": "GPT-5.2 — upgraded version of GPT-5 family with improved reasoning and multimodal capabilities. See provider docs for details.",
      "context_length": null,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000004",
        "completion": "0.00003"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-4o",
      "canonical_slug": "openai/gpt-4o",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-4o",
      "created": 1768409315,
      "description": "GPT-4o — general-purpose multimodal model from OpenAI.",
      "context_length": null,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000003",
        "completion": "0.00002"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-4o-mini-tts",
      "canonical_slug": "openai/gpt-4o-mini-tts",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-4o Mini TTS",
      "created": 1768409315,
      "description": "GPT-4o mini TTS — lightweight text-to-speech variant.",
      "context_length": null,
      "architecture": {
        "modality": "text->audio",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "audio"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000009",
        "completion": "0.000004",
        "audio": "0.000004"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-4o-mini",
      "canonical_slug": "openai/gpt-4o-mini",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-4o Mini",
      "created": 1768409315,
      "description": "GPT-4o Mini — cost-efficient variant of GPT-4o.",
      "context_length": null,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000008",
        "completion": "0.0000035"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-4",
      "canonical_slug": "openai/gpt-4",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-4",
      "created": 1692901234,
      "description": "GPT-4 is a large multimodal model that can solve difficult problems with greater accuracy.",
      "context_length": 8192,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT",
        "instruct_type": "chatml"
      },
      "pricing": {
        "prompt": "0.00003",
        "completion": "0.00006",
        "request": "0",
        "image": "0"
      },
      "top_provider": {
        "context_length": 8192,
        "max_completion_tokens": 4096,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty"
      ],
      "default_parameters": null,
      "expiration_date": null
    },
    {
      "id": "anthropic/claude-3.5-sonnet",
      "canonical_slug": "anthropic/claude-3.5-sonnet",
      "hugging_face_id": "",
      "name": "Anthropic: Claude 3.5 Sonnet",
      "created": 1768000000,
      "description": "Claude 3.5 Sonnet — Anthropic conversational model.",
      "context_length": 200000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00002",
        "completion": "0.00012"
      },
      "top_provider": {
        "context_length": 200000,
        "max_completion_tokens": 65536,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "anthropic/claude-3.5-sonnet:beta",
      "canonical_slug": "anthropic/claude-3.5-sonnet",
      "hugging_face_id": "",
      "name": "Anthropic: Claude 3.5 Sonnet (beta)",
      "created": 1768000000,
      "description": "Beta endpoint for Claude 3.5 Sonnet.",
      "context_length": 200000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00002",
        "completion": "0.00012"
      },
      "top_provider": {
        "context_length": 200000,
        "max_completion_tokens": 65536,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "google/gemini-pro",
      "canonical_slug": "google/gemini-pro",
      "hugging_face_id": "",
      "name": "Google: Gemini Pro",
      "created": 1767500000,
      "description": "Gemini Pro — Google’s high-end multimodal model.",
      "context_length": 1040000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000035",
        "completion": "0.000028"
      },
      "top_provider": {
        "context_length": 1040000,
        "max_completion_tokens": 32768,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs",
        "tools",
        "tool_choice"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "meta-llama/llama-3-70b-instruct",
      "canonical_slug": "meta-llama/llama-3-70b-instruct",
      "hugging_face_id": "",
      "name": "Meta Llama 3 70B Instruct",
      "created": 1767000000,
      "description": "Llama 3 70B instruct — Meta’s large instruct-tuned model.",
      "context_length": 262144,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000002",
        "completion": "0.000012"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 131072,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "mistralai/mixtral-8x7b-instruct",
      "canonical_slug": "mistralai/mixtral-8x7b-instruct",
      "hugging_face_id": "",
      "name": "Mistral: Mixtral 8x7B Instruct",
      "created": 1766900000,
      "description": "Mixtral — Mistral’s family of instruction tuned models.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000015",
        "completion": "0.000012"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "mistralai/mixtral-8x7b-instruct:free",
      "canonical_slug": "mistralai/mixtral-8x7b-instruct",
      "hugging_face_id": "",
      "name": "Mistral: Mixtral 8x7B Instruct (free)",
      "created": 1766900000,
      "description": "Free tier for Mixtral 8x7B Instruct.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "mistralai/mixtral-8x7b-instruct:beta",
      "canonical_slug": "mistralai/mixtral-8x7b-instruct",
      "hugging_face_id": "",
      "name": "Mistral: Mixtral 8x7B Instruct (beta)",
      "created": 1766900000,
      "description": "Beta variant for Mixtral 8x7B Instruct.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000015",
        "completion": "0.000012"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "mistralai/mixtral-8x7b-instruct:dev",
      "canonical_slug": "mistralai/mixtral-8x7b-instruct",
      "hugging_face_id": "",
      "name": "Mistral: Mixtral 8x7B Instruct (dev)",
      "created": 1766900000,
      "description": "Dev variant for Mixtral 8x7B Instruct.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000015",
        "completion": "0.000012"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "mistralai/mixtral-8x7b-instruct:free-v2",
      "canonical_slug": "mistralai/mixtral-8x7b-instruct",
      "hugging_face_id": "",
      "name": "Mistral: Mixtral 8x7B Instruct (free-v2)",
      "created": 1766900000,
      "description": "Free v2 for Mixtral 8x7B Instruct.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-4o-mini-tts:free",
      "canonical_slug": "openai/gpt-4o-mini-tts",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-4o Mini TTS (free)",
      "created": 1766800000,
      "description": "Free tier for GPT-4o Mini TTS.",
      "context_length": null,
      "architecture": {
        "modality": "text->audio",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "audio"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0",
        "audio": "0"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "cohere/command-r-08-2024",
      "canonical_slug": "cohere/command-r-08-2024",
      "hugging_face_id": "",
      "name": "Cohere: Command R (08/2024)",
      "created": 1766700000,
      "description": "Cohere Command R — retrieval-augmented model optimized for retrieval and RAG scenarios.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000006",
        "completion": "0.000004"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "deepseek/deepseek-r1",
      "canonical_slug": "deepseek/deepseek-r1",
      "hugging_face_id": "",
      "name": "DeepSeek: DeepSeek R1",
      "created": 1766600000,
      "description": "DeepSeek R1 — high-quality retrieval-first model.",
      "context_length": 65536,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000004",
        "completion": "0.000002"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "deepseek/deepseek-r1-distill-llama-70b",
      "canonical_slug": "deepseek/deepseek-r1-distill-llama-70b",
      "hugging_face_id": "",
      "name": "DeepSeek: DeepSeek R1 Distill Llama 70B",
      "created": 1766500000,
      "description": "Distilled variant of DeepSeek R1 optimized for latency.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000005",
        "completion": "0.000003"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "deepseek/deepseek-v3-base:free",
      "canonical_slug": "deepseek/deepseek-v3-base",
      "hugging_face_id": "",
      "name": "DeepSeek: DeepSeek V3 Base (free)",
      "created": 1766400000,
      "description": "Free tier for DeepSeek V3 Base.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "amazon/nova-lite-v1",
      "canonical_slug": "amazon/nova-lite-v1",
      "hugging_face_id": "",
      "name": "Amazon: Nova Lite v1",
      "created": 1766300000,
      "description": "Nova Lite v1 — efficient Amazon model for cost-sensitive workloads.",
      "context_length": 300000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000002",
        "completion": "0.000001"
      },
      "top_provider": {
        "context_length": 300000,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "amazon/nova-micro-v1",
      "canonical_slug": "amazon/nova-micro-v1",
      "hugging_face_id": "",
      "name": "Amazon: Nova Micro v1",
      "created": 1766200000,
      "description": "Nova Micro v1 — ultra low-cost Amazon model.",
      "context_length": 128000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.00000005",
        "completion": "0.0000002"
      },
      "top_provider": {
        "context_length": 128000,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "arion/arion-13b",
      "canonical_slug": "arion/arion-13b",
      "hugging_face_id": "",
      "name": "Arion: Arion 13B",
      "created": 1766100000,
      "description": "Arion 13B — balanced size/quality model for general-purpose tasks.",
      "context_length": 65536,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000003",
        "completion": "0.000002"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "aleph-alpha/luminoso-13b",
      "canonical_slug": "aleph-alpha/luminoso-13b",
      "hugging_face_id": "",
      "name": "Aleph Alpha: Luminoso 13B",
      "created": 1766000000,
      "description": "Luminoso 13B — Aleph Alpha’s 13B model.",
      "context_length": 65536,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000004",
        "completion": "0.0000025"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "writer/palmyra-x5:beta",
      "canonical_slug": "writer/palmyra-x5",
      "hugging_face_id": "",
      "name": "Writer: Palmyra X5 (beta)",
      "created": 1765900000,
      "description": "Beta snapshot for Palmyra X5.",
      "context_length": 1040000,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000006",
        "completion": "0.000006"
      },
      "top_provider": {
        "context_length": 1040000,
        "max_completion_tokens": 8192,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "google/gemini-ultra",
      "canonical_slug": "google/gemini-ultra",
      "hugging_face_id": "",
      "name": "Google: Gemini Ultra",
      "created": 1765800000,
      "description": "Gemini Ultra — largest Google model with extended context.",
      "context_length": 1040000,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000006",
        "completion": "0.00004"
      },
      "top_provider": {
        "context_length": 1040000,
        "max_completion_tokens": 65536,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs",
        "tools",
        "tool_choice"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "runway/mlv1",
      "canonical_slug": "runway/mlv1",
      "hugging_face_id": "",
      "name": "Runway: MLV1",
      "created": 1765700000,
      "description": "Runway MLV1 — multimodal model tuned for creative media workflows.",
      "context_length": 262144,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000007",
        "completion": "0.0000035"
      },
      "top_provider": {
        "context_length": 262144,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs",
        "tools",
        "tool_choice"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "stability/stablelm-3b",
      "canonical_slug": "stability/stablelm-3b",
      "hugging_face_id": "",
      "name": "Stability: StableLM 3B",
      "created": 1765600000,
      "description": "StableLM 3B — lightweight open-source conversational model.",
      "context_length": 32768,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000001",
        "completion": "0.0000005"
      },
      "top_provider": {
        "context_length": 32768,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "mistralai/mixtral-8x7b-instruct:inference-optimized",
      "canonical_slug": "mistralai/mixtral-8x7b-instruct",
      "hugging_face_id": "",
      "name": "Mistral: Mixtral 8x7B Instruct (inference-optimized)",
      "created": 1765500000,
      "description": "Inference-optimized variant of Mixtral 8x7B.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000012",
        "completion": "0.000009"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-4o:legacy",
      "canonical_slug": "openai/gpt-4o",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-4o (legacy)",
      "created": 1765400000,
      "description": "Legacy GPT-4o snapshot.",
      "context_length": null,
      "architecture": {
        "modality": "text+image->text",
        "input_modalities": [
          "text",
          "image"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000025",
        "completion": "0.00002"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-4o-mini:legacy",
      "canonical_slug": "openai/gpt-4o-mini",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-4o Mini (legacy)",
      "created": 1765300000,
      "description": "Legacy GPT-4o Mini snapshot.",
      "context_length": null,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000007",
        "completion": "0.0000035"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "openai/gpt-4o-mini:free",
      "canonical_slug": "openai/gpt-4o-mini",
      "hugging_face_id": "",
      "name": "OpenAI: GPT-4o Mini (free)",
      "created": 1764900000,
      "description": "Free tier for GPT-4o Mini.",
      "context_length": null,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "GPT",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0",
        "completion": "0"
      },
      "top_provider": {
        "context_length": null,
        "max_completion_tokens": null,
        "is_moderated": true
      },
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "replicate/stablelm-7b",
      "canonical_slug": "replicate/stablelm-7b",
      "hugging_face_id": "",
      "name": "Replicate: StableLM 7B",
      "created": 1765200000,
      "description": "Replicate-hosted StableLM 7B variant.",
      "context_length": 65536,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000003",
        "completion": "0.0000015"
      },
      "top_provider": {
        "context_length": 65536,
        "max_completion_tokens": 32768,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "carperai/llama-2-70b-chat",
      "canonical_slug": "carperai/llama-2-70b-chat",
      "hugging_face_id": "",
      "name": "CarperAI: LLaMA 2 70B Chat",
      "created": 1765100000,
      "description": "LLaMA 2 70B chat variant.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.0000015",
        "completion": "0.00001"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    },
    {
      "id": "mistralai/mixtral-8x7b-instruct:edge",
      "canonical_slug": "mistralai/mixtral-8x7b-instruct",
      "hugging_face_id": "",
      "name": "Mistral: Mixtral 8x7B Instruct (edge)",
      "created": 1765000000,
      "description": "Edge-optimized Mixtral 8x7B variant.",
      "context_length": 131072,
      "architecture": {
        "modality": "text->text",
        "input_modalities": [
          "text"
        ],
        "output_modalities": [
          "text"
        ],
        "tokenizer": "Other",
        "instruct_type": null
      },
      "pricing": {
        "prompt": "0.000001",
        "completion": "0.000008"
      },
      "top_provider": {
        "context_length": 131072,
        "max_completion_tokens": 65536,
        "is_moderated": false
      },
      "per_request_limits": null,
      "supported_parameters": [
        "temperature",
        "top_p",
        "max_tokens",
        "frequency_penalty",
        "presence_penalty",
        "stop",
        "structured_outputs"
      ],
      "default_parameters": {
        "temperature": null,
        "top_p": null,
        "frequency_penalty": null
      },
      "expiration_date": null
    }
  ]
}